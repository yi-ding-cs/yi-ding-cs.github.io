<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Welcom to Ding Yi's Homepage</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Ding Yi (丁一）</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://dyi1994.github.io/"><img src="photo.JPG" alt="alt text" width="180px" /></a>&nbsp;</td>
  <td align="left"><p>Ph.D. Candidate,<br />
Computational Intelligence Lab, <br />
School of Computer Science and Engineering <br />
Nanyang Technological University, <br />
Singapore <br /> 
Email: yi006@e.ntu.edu.sg <br />
<br />
<a href="https://www.researchgate.net/profile/Yi-Ding-113">[ResearchGate]</a> <a href="https://scholar.google.com/citations?user=8SYAgGIAAAAJ&hl=en">[Google Scholar]</a> 
  <a href="https://github.com/yi-ding-cs">[GitHub]</a></p>
</td></tr></table>
<h2>Biography</h2>
<p>I am currently a fourth-year Ph.D. student in the School of Computer Science and Engineering (SCSE), Nanyang Technological University, Singapore, supervised by <a href="https://personal.ntu.edu.sg/ctguan/">Prof Guan Cuntai</a>. 
Before that, I received my M.S.c in the School of Electrical and Electronics Engineering (EEE), Nanyang Technological University, Singapore in 2018, supervised by <a href="https://dr.ntu.edu.sg/cris/rp/rp00292"> Prof Chen Tupei</a>. 
Previously, I received my B.Eng degree from School of Information Science and Technology, Donghua University, Shanghai, China in 2017. <br /><br />
My research interests include brain-computer interface, deep/machine learning, graph neural networks, affective computing, computational neuroscience, 
and their applications on neural signal deconding and mental disorder regulation. I recently work on discrete and continuous emotion recognition using EEG, visual, and audio signals.</p>
<h2>News</h2>
<table class="imgtable"><tr><td>
<ul>
<li><p>[26-Apr-2022] One paper gets accepted by <i><b>IJCNN-22</b></i></p>
</li>
<li><p>[16-Apr-2022] One paper gets accepted by <i><b>IEEE Transactions on Affective Computing</b></i></p>
</li>
<li><p>[15-Apr-2022] One paper gets accepted by <i><b>CVPRW-22</b></i></p>
</li>
<li><p>[15-Apr-2022] Our team achieved the runner-up in the Valence-arousal estimation challenge of ABAW Competition held in <i><b>CVPR-22</b></i></p>
</ul>
</td></tr></table>
<h2>Publications</h2>
  <h3>Journal Papers</h3>
<ul>
<li><p><a href="https://arxiv.org/abs/2104.02935">TSception: Capturing Temporal Dynamics and Spatial Asymmetry from EEG for Emotion Recognition
</a> <br />
<b>Yi Ding</b>, Neethu Robinson, Su Zhang, Qiuhao Zeng, Cuntai Guan <br />
<i><b>IEEE Transactions on Affective Computing (TAC), 2022.</i></b>  [JCR Q1, IF=13.99] <a href="https://arxiv.org/pdf/2104.02935.pdf">[PDF]</a><a href="https://github.com/yi-ding-cs/TSception">[code]</a></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2105.02786">LGGNet: Learning from Local-Global-Graph Representations for Brain-Computer Interface</a> <br />
<b>Yi Ding</b>, Neethu Robinson, Qiuhao Zeng, Cuntai Guan <br />
<i><b>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), under revision. </b></i>[JCR Q1, IF=14.25] <a href="https://arxiv.org/pdf/2105.02786.pdf">[PDF]</a><a href="https://github.com/yi-ding-cs/LGG">[code]</a></p>
</li>
</ul>
<ul>
<li><p>MASA-TCN: Multi-anchor Space-aware Temporal Convolutional Neural Networks for Continuous and Discrete EEG Emotion Recognition <br />
<b>Yi Ding</b>*, Su Zhang*, Chuangao Tang, Cuntai Guan <br />
<i><b>Pattern Recognition (PR), under submission. </b></i>[JCR Q1, IF=7.74] *: equal contribution</p>
</li>
</ul>
  
<ul>
<li><p>Leader-follower Attentive Network for Continuous Emotion Recognition <br />
Su Zhang*, <b>Yi Ding</b>*, Chuangao Tang, Cuntai Guan <br />
<i><b>IEEE Transactions on Affective Computing (TAC), under review. </b></i>[JCR Q1, IF=13.99] *: equal contribution</p>
</li>
</ul>

<h3>Conference Papers</h3>
<ul>
<li><p> TESANet: Self-attention network for olfactory EEG classification <br />
  Chengxuan Tong, <b>Yi Ding</b>, Kevin Lim Jun Liang, Zhuo Zhang, Haihong Zhang, Cuntai Guan<br />
<i><b>International Joint Conference on Neural Networks (IJCNN) 2022. </b></i> </p>
</li>
</ul>

<ul>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Zhang_Continuous_Emotion_Recognition_Using_Visual-Audio-Linguistic_Information_A_Technical_Report_for_CVPRW_2022_paper.html">Continuous Emotion Recognition Using Visual-Audio-Linguistic Information: A Technical Report for ABAW3</a> <br />
Su Zhang, Ruyi An, <b>Yi Ding</b>, Cuntai Guan <br />
<i><b>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2022. </b></i> <a href="https://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Zhang_Continuous_Emotion_Recognition_Using_Visual-Audio-Linguistic_Information_A_Technical_Report_for_CVPRW_2022_paper.pdf">[PDF]</a><a href="https://github.com/sucv/ABAW3">[code]</a></p>
</li>
</ul>
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9629575"> Learning Generalized Representations of EEG between Multiple Cognitive Attention Tasks</a> <br />
<b>Yi Ding</b>, Nigel Wei Jun Ang, Aung Aung Phyo Wai, Cuntai Guan <br />
<i><b>43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), 2021. </b></i> <a href="https://paperhost.org/proceedings/embs/EMBC21/files/0591.pdf">[PDF]</a></p>
</li>
</ul>
<ul>
<li><p><a href="https://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Zhang_Continuous_Emotion_Recognition_With_Audio-Visual_Leader-Follower_Attentive_Fusion_ICCVW_2021_paper.html">Continuous Emotion Recognition With Audio-Visual Leader-Follower Attentive Fusion</a> <br />
Su Zhang*, <b>Yi Ding*</b>, Ziquan Wei*, Cuntai Guan <br />
<i><b>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, 2021. </b></i> <a href="https://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Zhang_Continuous_Emotion_Recognition_With_Audio-Visual_Leader-Follower_Attentive_Fusion_ICCVW_2021_paper.pdf">[PDF]</a><a href="https://github.com/sucv/ABAW2">[code]</a> *: equal contribution</p>
</li>
</ul>
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9206750">TSception:A Deep Learning Framework for Emotion Detection Using EEG</a> <br />
<b>Yi Ding</b>, Neethu Robinson, Qiuhao Zeng, Duo Chen, Aung Aung Phyo Wai, Tih-Shih Lee, Cuntai Guan<br />
<i><b>International Joint Conference on Neural Networks (IJCNN) 2020. </b></i> <a href="https://arxiv.org/pdf/2004.02965.pdf"> [PDF]</a><a href="https://github.com/deepBrains/TSception">[code]</a></p>
</li>
</ul>
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/8717063">Motor-Controlled Spindle (MCS) Detection for Primate in BCI System</a> <br />
Duo Chen, Rosa So, <b>Yi Ding</b>, Cuntai Guan<br />
  <i><b>International IEEE/EMBS Conference on Neural Engineering (NER), 2019. </b></i></p>
</li>
</ul>
<ul>
<li><p>Intracortical Activity decoding of motor imagery based on Deep Convolutional Neural Network: a Pilot Study. <br />
Duo Chen, Rosa So, <b>Yi Ding</b>, Cuntai Guan<br />
  <i><b>GBCIC, 2019. </b></i></p>
</li>
</ul>
<h2>Patents</h2>
Coming soon
<h2>Projects</h2>
Coming soon
<h2>Acadamic service</h2>
Coming soon
<h2>Teaching</h2>
Coming soon
</div>
</body>
</html>
